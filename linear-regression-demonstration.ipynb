{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebfc88fd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-05-18T18:36:23.155619Z",
     "iopub.status.busy": "2022-05-18T18:36:23.154578Z",
     "iopub.status.idle": "2022-05-18T18:36:23.166672Z",
     "shell.execute_reply": "2022-05-18T18:36:23.165896Z"
    },
    "papermill": {
     "duration": 0.04339,
     "end_time": "2022-05-18T18:36:23.169630",
     "exception": false,
     "start_time": "2022-05-18T18:36:23.126240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82d5f2c",
   "metadata": {
    "papermill": {
     "duration": 0.024431,
     "end_time": "2022-05-18T18:36:23.221515",
     "exception": false,
     "start_time": "2022-05-18T18:36:23.197084",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Linear regression with sklearn API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c535a",
   "metadata": {
    "papermill": {
     "duration": 0.024988,
     "end_time": "2022-05-18T18:36:23.271298",
     "exception": false,
     "start_time": "2022-05-18T18:36:23.246310",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The objective of this notebook is to demonstrate how to build a linear regression model with sklearn. We will be using the following set up:\n",
    "1. Dataset: California housing\n",
    "2. Linear regression API: LinearRegression\n",
    "3. Training: `fit` (normal equation) and `cross_validate`(normal equation with cross validation).\n",
    "4. Evaluation: `score`(r2 score) and `cross_val_score` with different scoring parameters.\n",
    "\n",
    "We will study the model dianosis with `LearningCurve`  and learn how to examine the learned model or weight vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbc6e47b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T18:36:23.322847Z",
     "iopub.status.busy": "2022-05-18T18:36:23.321902Z",
     "iopub.status.idle": "2022-05-18T18:36:24.852667Z",
     "shell.execute_reply": "2022-05-18T18:36:24.851602Z"
    },
    "papermill": {
     "duration": 1.559105,
     "end_time": "2022-05-18T18:36:24.855041",
     "exception": false,
     "start_time": "2022-05-18T18:36:23.295936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing packeges for manipulation and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# importing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression# linear regression api\n",
    "\n",
    "# importing models\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit#split in cross_validation\n",
    "\n",
    "# importing metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# importing pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad41779",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T18:36:24.910881Z",
     "iopub.status.busy": "2022-05-18T18:36:24.910428Z",
     "iopub.status.idle": "2022-05-18T18:36:24.914605Z",
     "shell.execute_reply": "2022-05-18T18:36:24.913933Z"
    },
    "papermill": {
     "duration": 0.035381,
     "end_time": "2022-05-18T18:36:24.916598",
     "exception": false,
     "start_time": "2022-05-18T18:36:24.881217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(306)# setting seed to get repeatability across diff. runs\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e45b6f",
   "metadata": {
    "papermill": {
     "duration": 0.024909,
     "end_time": "2022-05-18T18:36:24.967430",
     "exception": false,
     "start_time": "2022-05-18T18:36:24.942521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will use ShuffleSplit cross validation with:\n",
    "* 10 folds(n_splits) and\n",
    "* set aside 20% examples as test examples(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f7d00e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T18:36:25.018588Z",
     "iopub.status.busy": "2022-05-18T18:36:25.017806Z",
     "iopub.status.idle": "2022-05-18T18:36:25.021835Z",
     "shell.execute_reply": "2022-05-18T18:36:25.021219Z"
    },
    "papermill": {
     "duration": 0.031906,
     "end_time": "2022-05-18T18:36:25.024039",
     "exception": false,
     "start_time": "2022-05-18T18:36:24.992133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shuffle_split_cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "# create 10 folds through shuffle split by keeping aside 20% examples as  test in each fold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a07b929",
   "metadata": {
    "papermill": {
     "duration": 0.025056,
     "end_time": "2022-05-18T18:36:25.075619",
     "exception": false,
     "start_time": "2022-05-18T18:36:25.050563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# STEP #1: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79a70f7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T18:36:25.127311Z",
     "iopub.status.busy": "2022-05-18T18:36:25.126706Z",
     "iopub.status.idle": "2022-05-18T18:36:26.860341Z",
     "shell.execute_reply": "2022-05-18T18:36:26.859362Z"
    },
    "papermill": {
     "duration": 1.762267,
     "end_time": "2022-05-18T18:36:26.862702",
     "exception": false,
     "start_time": "2022-05-18T18:36:25.100435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features, labels = fetch_california_housing(as_frame=True, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d636159",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T18:36:26.913565Z",
     "iopub.status.busy": "2022-05-18T18:36:26.913136Z",
     "iopub.status.idle": "2022-05-18T18:36:26.918421Z",
     "shell.execute_reply": "2022-05-18T18:36:26.917779Z"
    },
    "papermill": {
     "duration": 0.033623,
     "end_time": "2022-05-18T18:36:26.921066",
     "exception": false,
     "start_time": "2022-05-18T18:36:26.887443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (20640, 8)\n",
      "Shape of label vector: (20640,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of feature matrix:\", features.shape)\n",
    "print(\"Shape of label vector:\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64562fe3",
   "metadata": {
    "papermill": {
     "duration": 0.024839,
     "end_time": "2022-05-18T18:36:26.971510",
     "exception": false,
     "start_time": "2022-05-18T18:36:26.946671",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As a sanity check, make sure that the number of rows in feature matrix and labels match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b23fce0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T18:36:27.024919Z",
     "iopub.status.busy": "2022-05-18T18:36:27.024313Z",
     "iopub.status.idle": "2022-05-18T18:36:27.030287Z",
     "shell.execute_reply": "2022-05-18T18:36:27.028799Z"
    },
    "papermill": {
     "duration": 0.035087,
     "end_time": "2022-05-18T18:36:27.032512",
     "exception": false,
     "start_time": "2022-05-18T18:36:26.997425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20640\n",
      "20640\n"
     ]
    }
   ],
   "source": [
    "assert (features.shape[0] == labels.shape[0])\n",
    "print(features.shape[0])\n",
    "print(labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcb5ef7",
   "metadata": {
    "papermill": {
     "duration": 0.025008,
     "end_time": "2022-05-18T18:36:27.083929",
     "exception": false,
     "start_time": "2022-05-18T18:36:27.058921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# STEP #2: Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5809a6",
   "metadata": {
    "papermill": {
     "duration": 0.025172,
     "end_time": "2022-05-18T18:36:27.134949",
     "exception": false,
     "start_time": "2022-05-18T18:36:27.109777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1ee5199",
   "metadata": {
    "papermill": {
     "duration": 0.026589,
     "end_time": "2022-05-18T18:36:27.186844",
     "exception": false,
     "start_time": "2022-05-18T18:36:27.160255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# STEP #3: Preprocessing and model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce50f7c7",
   "metadata": {
    "papermill": {
     "duration": 0.02588,
     "end_time": "2022-05-18T18:36:27.238913",
     "exception": false,
     "start_time": "2022-05-18T18:36:27.213033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.1 Train and test split\n",
    "\n",
    "The first step is to split the training data into training and test set. We do not access the test data till the end. All data exploration and tuning is performed on the training set and by setting aside a small portion of training as a dev or validation set.\n",
    "\n",
    "The following code snippet divides the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d802cfb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T18:36:27.291493Z",
     "iopub.status.busy": "2022-05-18T18:36:27.290761Z",
     "iopub.status.idle": "2022-05-18T18:36:27.298924Z",
     "shell.execute_reply": "2022-05-18T18:36:27.298157Z"
    },
    "papermill": {
     "duration": 0.036781,
     "end_time": "2022-05-18T18:36:27.301088",
     "exception": false,
     "start_time": "2022-05-18T18:36:27.264307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2c04a63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T18:36:27.353878Z",
     "iopub.status.busy": "2022-05-18T18:36:27.353266Z",
     "iopub.status.idle": "2022-05-18T18:36:27.359425Z",
     "shell.execute_reply": "2022-05-18T18:36:27.358055Z"
    },
    "papermill": {
     "duration": 0.035455,
     "end_time": "2022-05-18T18:36:27.362066",
     "exception": false,
     "start_time": "2022-05-18T18:36:27.326611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training samples: 15480\n",
      "# test samples: 5160\n"
     ]
    }
   ],
   "source": [
    "# let's examine the shapes of training and test sets\n",
    "print (\"# training samples:\", train_features.shape[0])\n",
    "print(\"# test samples:\", test_features.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f4c6a8",
   "metadata": {
    "papermill": {
     "duration": 0.026548,
     "end_time": "2022-05-18T18:36:27.415713",
     "exception": false,
     "start_time": "2022-05-18T18:36:27.389165",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It's time to perform another sanity check-here we check if the training feature matrix has the same number of rows as the training label vector. We perform the same check on test set too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "977ab3b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T18:36:27.470168Z",
     "iopub.status.busy": "2022-05-18T18:36:27.469842Z",
     "iopub.status.idle": "2022-05-18T18:36:27.474770Z",
     "shell.execute_reply": "2022-05-18T18:36:27.474076Z"
    },
    "papermill": {
     "duration": 0.034503,
     "end_time": "2022-05-18T18:36:27.476981",
     "exception": false,
     "start_time": "2022-05-18T18:36:27.442478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert (train_features.shape[0] == train_labels.shape[0])\n",
    "assert (test_features.shape[0] == test_labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c181e4ae",
   "metadata": {
    "papermill": {
     "duration": 0.025707,
     "end_time": "2022-05-18T18:36:27.529456",
     "exception": false,
     "start_time": "2022-05-18T18:36:27.503749",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.2 Pipeline: preprocessing + model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511594b9",
   "metadata": {
    "papermill": {
     "duration": 0.025795,
     "end_time": "2022-05-18T18:36:27.581370",
     "exception": false,
     "start_time": "2022-05-18T18:36:27.555575",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As a first step, build linear regression models with default parameter setting of `LinearRegression` APIs.\n",
    "> We will make use of `Pipeline` API for combining data preprocessing and model building.\n",
    "\n",
    "We will use `StandardScaler` feature scaling to bring all features on the same scale followed by a `LinearRegression` model.\n",
    "The `pipeline` object has two components:\n",
    "   1. `StandardScaler`  as step 1\n",
    "   2. `LinearRegression` as step 2\n",
    "   \n",
    "After constructing the pipeline object, let's train it with training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39c7cb8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T18:36:27.635281Z",
     "iopub.status.busy": "2022-05-18T18:36:27.634729Z",
     "iopub.status.idle": "2022-05-18T18:36:27.671583Z",
     "shell.execute_reply": "2022-05-18T18:36:27.670700Z"
    },
    "papermill": {
     "duration": 0.067053,
     "end_time": "2022-05-18T18:36:27.674368",
     "exception": false,
     "start_time": "2022-05-18T18:36:27.607315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feature scaling', StandardScaler()),\n",
       "                ('lin_reg', LinearRegression())])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the linear regression model.\n",
    "lin_reg_pipeline = Pipeline([(\"feature scaling\", StandardScaler()),\n",
    "                             (\"lin_reg\", LinearRegression())])\n",
    "\n",
    "# train linear regression model with normal equation.\n",
    "lin_reg_pipeline.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc87ff",
   "metadata": {
    "papermill": {
     "duration": 0.027442,
     "end_time": "2022-05-18T18:36:27.728591",
     "exception": false,
     "start_time": "2022-05-18T18:36:27.701149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now that we have trained the model, let's check the learnt/estimated weight vectors (`intercept_` and `coef_`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce05fe1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T18:36:27.784037Z",
     "iopub.status.busy": "2022-05-18T18:36:27.783508Z",
     "iopub.status.idle": "2022-05-18T18:36:27.790052Z",
     "shell.execute_reply": "2022-05-18T18:36:27.789078Z"
    },
    "papermill": {
     "duration": 0.036186,
     "end_time": "2022-05-18T18:36:27.792019",
     "exception": false,
     "start_time": "2022-05-18T18:36:27.755833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept (w_0): 2.0703489205426377\n",
      "weight vector (w_1, ... , w_m): [ 0.85210815  0.12065533 -0.30210555  0.34860575 -0.00164465 -0.04116356\n",
      " -0.89314697 -0.86784046]\n"
     ]
    }
   ],
   "source": [
    "print(\"intercept (w_0):\", lin_reg_pipeline[-1].intercept_)\n",
    "print(\"weight vector (w_1, ... , w_m):\", lin_reg_pipeline[-1].coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7032779",
   "metadata": {
    "papermill": {
     "duration": 0.026446,
     "end_time": "2022-05-18T18:36:27.845244",
     "exception": false,
     "start_time": "2022-05-18T18:36:27.818798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A couple of things to notice:\n",
    "* We accessed the LinearRegression object as `lin_reg_pipeline[-1]` which is the last step in the pipeline.\n",
    "* The intercept can be obtained via `intercept_` member variable and \n",
    "* The weight vector corresponding to features via `coef_`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf23dbd9",
   "metadata": {
    "papermill": {
     "duration": 0.026404,
     "end_time": "2022-05-18T18:36:27.898409",
     "exception": false,
     "start_time": "2022-05-18T18:36:27.872005",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# STEP #4: Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc34648",
   "metadata": {
    "papermill": {
     "duration": 0.026968,
     "end_time": "2022-05-18T18:36:27.952066",
     "exception": false,
     "start_time": "2022-05-18T18:36:27.925098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## `Score`\n",
    "\n",
    "Let's use `score` method to obtain train and test errors with twin objectives\n",
    "> * Estimation of model performance as provided by test errror.\n",
    "> * Comparison of errors for model diagnostic purpose(under/over/just right fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ca504b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-18T18:36:28.008116Z",
     "iopub.status.busy": "2022-05-18T18:36:28.007829Z",
     "iopub.status.idle": "2022-05-18T18:36:28.025071Z",
     "shell.execute_reply": "2022-05-18T18:36:28.024234Z"
    },
    "papermill": {
     "duration": 0.049073,
     "end_time": "2022-05-18T18:36:28.028504",
     "exception": false,
     "start_time": "2022-05-18T18:36:27.979431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance on test set:  0.5910509795491351\n",
      "Model performance on train set:  0.609873031052925\n"
     ]
    }
   ],
   "source": [
    "# evaluate model performance on the test set.\n",
    "test_score = lin_reg_pipeline.score(test_features, test_labels)\n",
    "print(\"Model performance on test set: \", test_score)\n",
    "\n",
    "train_score = lin_reg_pipeline.score(train_features, \n",
    "                                    train_labels)\n",
    "print(\"Model performance on train set: \", train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3094605c",
   "metadata": {
    "papermill": {
     "duration": 0.049096,
     "end_time": "2022-05-18T18:36:28.127341",
     "exception": false,
     "start_time": "2022-05-18T18:36:28.078245",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The `score` method returns `r2` score whose best value is 1. The `r2` scores on training and test are comparable but they are not that high. It points to underfitting issue in model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bfb85b",
   "metadata": {
    "papermill": {
     "duration": 0.0276,
     "end_time": "2022-05-18T18:36:28.188906",
     "exception": false,
     "start_time": "2022-05-18T18:36:28.161306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15.921043,
   "end_time": "2022-05-18T18:36:28.936485",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-18T18:36:13.015442",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
